{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUQTzCrWIW-U"
      },
      "source": [
        "# ***Research Paper Assistant***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKGKs4UMIcHJ",
        "outputId": "37b6eba2-c49a-4573-9023-57583971c128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.5)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (4.1.2)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.5.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.56.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.12.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.45.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-core langchain-community langchain-google-genai langchain-huggingface langchain-text-splitters sentence-transformers pypdf faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MMmmTj1XIlMR",
        "outputId": "e7241b5e-33f7-4a28-ae02-183fef4e6a46"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d6f646d0-1920-428e-bd3a-b5bfc4a62c6b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d6f646d0-1920-428e-bd3a-b5bfc4a62c6b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Journal Paper (1).pdf to Journal Paper (1).pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUzje7WcIwLf"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"]=\"\"\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"\"\n",
        "\n",
        "model=ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=1024\n",
        ")\n",
        "embedding = HuggingFaceEmbeddings(\n",
        "    model_name=\"intfloat/e5-large-v2\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "loader=PyPDFLoader(\"Journal Paper (1).pdf\")\n",
        "docs=loader.load()\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(\n",
        "        f\"(Page {doc.metadata.get('page', 'N/A')}): {doc.page_content}\"\n",
        "        for doc in docs\n",
        "    )\n",
        "\n",
        "parser=StrOutputParser()\n",
        "splitter=RecursiveCharacterTextSplitter(chunk_size=1200,chunk_overlap=350)\n",
        "chunk=splitter.split_documents(docs)\n",
        "\n",
        "vectorStore=FAISS.from_documents(chunk,embedding)\n",
        "retriever=vectorStore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 8, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
        ")\n",
        "\n",
        "template = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a research-focused AI assistant.\n",
        "\n",
        "Your task is to answer the user's question **strictly using the provided context** extracted from an academic paper.\n",
        "\n",
        "Instructions:\n",
        "- Use only the information present in the context.\n",
        "- If the answer is not present in the context, say:\n",
        "  \"The provided document does not contain sufficient information to answer this question.\"\n",
        "- Use a clear, academic, and concise tone.\n",
        "- When relevant, reference sections such as Abstract, Methodology, Results, or Conclusion.\n",
        "- If equations, algorithms, or experimental results are mentioned in the context, include them in your explanation.\n",
        "- Do not add external knowledge or assumptions.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "def RAG():\n",
        "  while True:\n",
        "    question=input(\"You : \")\n",
        "    if question.lower()==\"exit\":\n",
        "      return\n",
        "    ansDoc=retriever.invoke(question)\n",
        "    context=format_docs(ansDoc)\n",
        "    chain=template | model | parser\n",
        "    result=chain.invoke({\n",
        "        \"context\":context,\n",
        "        \"question\":question\n",
        "    })\n",
        "    print()\n",
        "    print(f\"Bot : {result}\")\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1BnVxvMLLbx"
      },
      "source": [
        "***Sample Questions***\n",
        "\n",
        "*1. What are contents present in this paper?*\n",
        "\n",
        "*2. can u explain all the contents in detail*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq_1iu0dI2jv",
        "outputId": "d95e7ae6-6b1f-4ec2-9a15-060cd371e172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You : What are contents present in this paper?\n",
            "\n",
            "Bot : This paper presents a real-time facial emotion recognition (FER) system integrated into a chat application to enhance emotional expressiveness during digital communication (Page 0, Page 4 - V. CONCLUSION AND FUTURE SCOPE).\n",
            "\n",
            "The contents of the paper include:\n",
            "*   **Problem Statement and Motivation:** It addresses the limitation of traditional text-based communication lacking emotional indicators, which leads to misunderstandings, emphasizing the need for emotionally intelligent interaction in human-computer interaction (HCI) (Page 0).\n",
            "*   **System Overview:** The suggested system offers a real-time facial emotion recognition function as an add-on module (Page 0).\n",
            "*   **Methodology:**\n",
            "    *   **Dataset:** The Cohn-Kanade Plus (CK+) dataset [3] was used for training and assessment, featuring high-resolution photos with emotion labels (happiness, surprise, anger, contempt, disgust, sadness, and fear) and Action Units (Page 2 - B. Description of the Dataset(CK+ dataset)).\n",
            "    *   **Facial Landmark Detection:** MediaPipe's Face Mesh was utilized to extract 468 unique coordinate points (landmarks) from each image, which are then normalized for scale invariance (Page 2 - C. Detecting Facial Landmarks with MediaPipe, Page 4 - V. CONCLUSION AND FUTURE SCOPE).\n",
            "    *   **Feature Extraction:** Geometric and ratio-based features, such as mouth aspect ratio (MAR), eye aspect ratio (EAR), mouth width, mouth height, eye openness, jaw drop, symmetry of mouth corners, eyebrow slant, eyebrow raise, and nose-mouth distance, were calculated from the detected facial landmarks. A total of 14 features were extracted (Page 2 - D. Extraction of Features).\n",
            "    *   **Deep Learning Model Architecture:** A fully connected feed-forward neural network implemented with TensorFlow and Keras. It consists of an input layer for the 14 extracted features, two hidden layers (64 neurons with Relu activation, followed by a dropout layer; 32 neurons, followed by another dropout layer), and an output layer (seven emotion classes) with a softmax activation function. The model was trained for 60 epochs using the Adam optimizer and sparse categorical cross-entropy loss function, with class weighting to address data imbalance (Page 2 - E. Architecture of the Deep Learning Model).\n",
            "*   **Experimental Results:**\n",
            "    *   The model demonstrated efficient learning and little overfitting, with training and validation loss steadily declining over 60 epochs (Page 3).\n",
            "    *   It showed a high degree of generalization across various subjects and emotion types (Page 3).\n",
            "    *   Emotions like happiness and surprise were classified with high precision, while visually subtle emotions such as disgust and contempt occasionally showed misclassifications (Page 3).\n",
            "    *   The classification report verified balanced recall and precision for most categories (Page 3).\n",
            "    *   Performance was visually assessed using heatmaps and training curves plotted with Matplotlib and Seaborn (Page 3).\n",
            "    *   The model achieved an accuracy of 85.28% in identifying seven main emotions (Page 4 - V. CONCLUSION AND FUTURE SCOPE).\n",
            "    *   Real-time testing showed low latency, with an average emotion detection and display delay of less than 300 milliseconds (Page 4).\n",
            "*   **Conclusion:** The paper concludes that a compact neural network combined with geometric facial features provides a dependable, portable, and effective real-time facial emotion recognition solution for interactive chat systems, mapping emotions to emojis to bridge the emotional divide in text-based communication (Page 4).\n",
            "*   **Future Scope:** The system can be extended to include voice and text sentiment analysis for multimodal emotion detection (Page 4 - V. CONCLUSION AND FUTURE SCOPE).\n",
            "*   **References:** A list of references is provided, including works on deep facial expression recognition, the CK+ dataset, real-time emotion detection, integrating emotion recognition systems into HCI, and MediaPipe's Face Mesh (Page 4 - VII. REFERENCES).\n",
            "\n",
            "\n",
            "\n",
            "You :  can u explain all the contents in detail\n",
            "\n",
            "Bot : The provided document details a system for emotion detection in text-based communication, its methodology, architecture, and future enhancements.\n",
            "\n",
            "**I. System Overview and Features**\n",
            "The system aims to bridge the emotional divide in text-based communication by adding an empathic layer that maps detected emotions to emojis. It operates with an accuracy of 85.28% in emotion detection (Page 4). The design ensures asynchronous processing, allowing emotion detection to operate in parallel with chat communication to maintain low latency and real-time responsiveness. Computational efficiency is achieved by utilizing lightweight geometric features instead of pixel-based inputs (Page 1).\n",
            "\n",
            "**II. Methodology**\n",
            "\n",
            "*   **A. Dataset (Section B, Page 2):** The Cohn-Kanade Plus (CK+) dataset [3] was used for training and assessment. This dataset includes high-resolution images depicting emotional states like happiness, surprise, anger, contempt, disgust, sadness, and fear. It comprises 593 video sequences from 123 subjects and includes Action Units for precise emotion classification. Each sequence progresses from a neutral expression to a peak emotional state.\n",
            "*   **B. Preprocessing (Page 2):** Images were resized for consistency and converted to grayscale. Captured frames are converted from BGR to RGB format before processing.\n",
            "*   **C. Detecting Facial Landmarks with MediaPipe (Section C, Page 2):** MediaPipe's Face Mesh model was used to extract 468 unique coordinate points (landmarks) per face. This model recognizes facial landmarks even with variations in lighting, angle, and facial orientation. These coordinates are then normalized to ensure scale invariance.\n",
            "*   **D. Extraction of Features (Section D, Page 2):** From the detected facial landmarks, 14 geometric and ratio-based features are extracted. These include different aspect ratios and Euclidean distances between key facial points related to the jaw, eyes, mouth, and eyebrows. Examples are the mouth aspect ratio (MAR) and eye aspect ratio (EAR), as well as parameters like mouth width, mouth height, eye openness, jaw drop, symmetry of mouth corners, eyebrow slant, eyebrow raise, and nose-mouth distance. These features are normalized and were chosen for their balance between interpretability and computational efficiency.\n",
            "*   **E. Architecture of the Deep Learning Model (Section E, Page 2):** The model is a fully connected feed-forward neural network built with TensorFlow and Keras. Its input layer represents the 14 extracted features, followed by two hidden layers and an output layer.\n",
            "    *   The first hidden layer has 64 neurons with ReLU activation, followed by a dropout layer to prevent overfitting.\n",
            "    *   The second hidden layer has 32 neurons, also followed by a dropout layer for improved generalization.\n",
            "    *   The output layer uses a softmax activation function to generate probability distributions for the seven emotion classes.\n",
            "    *   The model is trained using the Adam optimizer with a sparse categorical cross-entropy loss function. Training involved 60 epochs, with class weighting applied to address data imbalance across emotion categories.\n",
            "*   **F. Evaluation and Results (Page 3, Page 4):** The model's performance was evaluated using metrics such as accuracy, precision, recall, and F1 score. The confusion matrix indicated high accuracy for detecting happiness and surprise, with moderate accuracy for complex emotions like contempt and disgust. Accuracy and loss curves were plotted to monitor training progress and identify potential overfitting. The overall system achieved an accuracy of 85.28% for emotion detection (Page 4).\n",
            "*   **G. Integrating the model into Chat Application (Section G, Page 3):** The trained deep learning model is integrated into a real-time chat system. This application manages user authentication, message storage, and data streaming, leveraging contemporary web technologies and Supabase for database synchronization. The emotion recognition feature is implemented via a frontend extension that uses browser APIs to communicate with the user's webcam. During a chat session, the system records live frames, performs local facial landmark detection, and sends the extracted feature vector to the trained model.\n",
            "\n",
            "**III. Future Work (Page 4)**\n",
            "Future improvements include adding voice and text sentiment analysis for multimodal emotion detection. Performance and scalability can be further enhanced through optimization techniques such as model pruning, lighting adaptation, and cross-dataset training.\n",
            "\n",
            "**IV. Acknowledgment (Section VI, Page 4)**\n",
            "The researchers acknowledge the support, direction, and encouragement from T. John Institute of Technology's Department of Computer Science and Engineering, with special thanks to Dr. Sudaroli (project guide) and other faculty members for their insights on system integration and facial emotion recognition.\n",
            "\n",
            "**V. References (Section VII, Page 4)**\n",
            "The document includes a list of references [1-7] relevant to deep facial expression recognition, datasets, real-time emotion detection, and deep learning.\n",
            "\n",
            "\n",
            "\n",
            "You :  how are the landmarks detected\n",
            "\n",
            "Bot : Facial landmarks are detected using MediaPipe's Face Mesh, a real-time framework developed by Google Research [7], which provides 468 high-fidelity 3D facial landmarks per face (Page 2, Section C).\n",
            "\n",
            "The process involves:\n",
            "1.  Processing the webcam stream frame by frame.\n",
            "2.  Converting the captured image from BGR to RGB format to meet MediaPipe's processing requirements (Page 2, Section C).\n",
            "3.  The Face Mesh model then recognizes facial landmarks, even with variations in lighting, angle, and facial orientation (Page 2, Section C).\n",
            "4.  These coordinate points are subsequently normalized to ensure scale invariance across different users and distances from the camera (Page 2, Section C).\n",
            "\n",
            "\n",
            "\n",
            "You : exit\n"
          ]
        }
      ],
      "source": [
        "RAG()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
